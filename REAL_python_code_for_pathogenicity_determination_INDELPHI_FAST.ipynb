{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mlcsmits/Alternative-Splicing-MBI03/blob/main/REAL_python_code_for_pathogenicity_determination_INDELPHI_FAST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this cell, press play button"
      ],
      "metadata": {
        "id": "jrWvN9b_IaKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "####    USE ONLY THE FIRST TIME!\n",
        "!pip install beautifulsoup4 requests aiohttp nest_asyncio lxml selenium webdriver-manager biopython\n",
        "\n",
        "!apt-get update\n",
        "!apt install -y chromium-browser\n",
        "!pip install -q selenium\n",
        "!pip install -q webdriver-manager"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfSapiJ12IIc",
        "outputId": "f44bac5c-f4f8-478c-cee4-a2af273813d0",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (3.10.9)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (4.9.4)\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.25.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting webdriver-manager\n",
            "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting biopython\n",
            "  Downloading biopython-1.84-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (4.0.3)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.26.2-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Collecting python-dotenv (from webdriver-manager)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from webdriver-manager) (24.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.26.4)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Downloading selenium-4.25.0-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
            "Downloading biopython-1.84-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.26.2-py3-none-any.whl (475 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m476.0/476.0 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-dotenv, outcome, h11, biopython, wsproto, webdriver-manager, trio, trio-websocket, selenium\n",
            "Successfully installed biopython-1.84 h11-0.14.0 outcome-1.3.0.post0 python-dotenv-1.0.1 selenium-4.25.0 trio-0.26.2 trio-websocket-0.11.1 webdriver-manager-4.0.2 wsproto-1.2.0\n",
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Ign:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,160 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,326 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,380 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,602 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,594 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,449 kB]\n",
            "Fetched 18.8 MB in 4s (4,542 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  apparmor libfuse3-3 liblzo2-2 libudev1 snapd squashfs-tools systemd-hwe-hwdb udev\n",
            "Suggested packages:\n",
            "  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n",
            "The following NEW packages will be installed:\n",
            "  apparmor chromium-browser libfuse3-3 liblzo2-2 snapd squashfs-tools systemd-hwe-hwdb udev\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 8 newly installed, 0 to remove and 50 not upgraded.\n",
            "Need to get 28.5 MB of archives.\n",
            "After this operation, 117 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.4 [598 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblzo2-2 amd64 2.10-2build3 [53.7 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.12 [78.2 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.12 [1,557 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfuse3-3 amd64 3.10.5-1build1 [81.2 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 snapd amd64 2.63+22.04ubuntu0.1 [25.9 MB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [49.2 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.5 [3,228 B]\n",
            "Fetched 28.5 MB in 2s (14.7 MB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package apparmor.\n",
            "(Reading database ... 123621 files and directories currently installed.)\n",
            "Preparing to unpack .../apparmor_3.0.4-2ubuntu2.4_amd64.deb ...\n",
            "Unpacking apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Selecting previously unselected package liblzo2-2:amd64.\n",
            "Preparing to unpack .../liblzo2-2_2.10-2build3_amd64.deb ...\n",
            "Unpacking liblzo2-2:amd64 (2.10-2build3) ...\n",
            "Selecting previously unselected package squashfs-tools.\n",
            "Preparing to unpack .../squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n",
            "Unpacking squashfs-tools (1:4.5-3build1) ...\n",
            "Preparing to unpack .../libudev1_249.11-0ubuntu3.12_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (249.11-0ubuntu3.12) over (249.11-0ubuntu3.10) ...\n",
            "Setting up libudev1:amd64 (249.11-0ubuntu3.12) ...\n",
            "Selecting previously unselected package udev.\n",
            "(Reading database ... 123829 files and directories currently installed.)\n",
            "Preparing to unpack .../udev_249.11-0ubuntu3.12_amd64.deb ...\n",
            "Unpacking udev (249.11-0ubuntu3.12) ...\n",
            "Selecting previously unselected package libfuse3-3:amd64.\n",
            "Preparing to unpack .../libfuse3-3_3.10.5-1build1_amd64.deb ...\n",
            "Unpacking libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Selecting previously unselected package snapd.\n",
            "Preparing to unpack .../snapd_2.63+22.04ubuntu0.1_amd64.deb ...\n",
            "Unpacking snapd (2.63+22.04ubuntu0.1) ...\n",
            "Setting up apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n",
            "Setting up liblzo2-2:amd64 (2.10-2build3) ...\n",
            "Setting up squashfs-tools (1:4.5-3build1) ...\n",
            "Setting up udev (249.11-0ubuntu3.12) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Setting up snapd (2.63+22.04ubuntu0.1) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service → /lib/systemd/system/snapd.apparmor.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service → /lib/systemd/system/snapd.autoimport.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service → /lib/systemd/system/snapd.core-fixup.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service → /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Unit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service → /lib/systemd/system/snapd.service.\n",
            "Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer → /lib/systemd/system/snapd.snap-repair.timer.\n",
            "Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket → /lib/systemd/system/snapd.socket.\n",
            "Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service → /lib/systemd/system/snapd.system-shutdown.service.\n",
            "Selecting previously unselected package chromium-browser.\n",
            "(Reading database ... 124059 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "=> Installing the chromium snap\n",
            "==> Checking connectivity with the snap store\n",
            "===> System doesn't have a working snapd, skipping\n",
            "Unpacking chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package systemd-hwe-hwdb.\n",
            "Preparing to unpack .../systemd-hwe-hwdb_249.11.5_all.deb ...\n",
            "Unpacking systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n",
            "Processing triggers for udev (249.11-0ubuntu3.12) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Don't run this three cells"
      ],
      "metadata": {
        "id": "pOpqWffmIhKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import csv\n",
        "import aiohttp\n",
        "import asyncio\n",
        "from bs4 import BeautifulSoup\n",
        "import nest_asyncio\n",
        "import requests\n",
        "import time\n",
        "import os\n",
        "filename = input(\"insert file name:\")\n",
        "try:\n",
        "  os.remove('/content/temp.csv')\n",
        "except FileNotFoundError: print(f\"File '/content/temp.csv' not found.\")\n",
        "# Fix for nested event loops (necessary in Jupyter)\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Debugging function for easier printing\n",
        "def debug(msg):\n",
        "    print(f\"[DEBUG] {msg}\")\n",
        "try:\n",
        "  os.remove('/content/input_urls.csv')\n",
        "except FileNotFoundError: print(f\"File '/content/input_urls.csv' not found.\")\n",
        "with open('/content/input_urls.csv', 'a') as bla:\n",
        "    bla.write('Link for Clinical Significance\\n')\n",
        "\n",
        "# Function to read SNP locations from a CSV file\n",
        "def read_snp_locations(file_path):\n",
        "    snp_locations = []\n",
        "    with open(file_path, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        next(reader)  # Skip header\n",
        "        for row in reader:\n",
        "            if row and row[-1]:  # Ensure the last column is not empty\n",
        "                snp_locations.append(row[-1].strip())\n",
        "    return snp_locations\n",
        "\n",
        "# Function to write URLs to a CSV file\n",
        "def write_urls_to_csv(file_path, urls):\n",
        "    with open(file_path, 'a', newline='') as out_file:\n",
        "        writer = csv.writer(out_file)\n",
        "        for url in urls:\n",
        "            writer.writerow([url])\n",
        "\n",
        "# Read SNP locations and write to temp CSV\n",
        "snp_locations = read_snp_locations(f'/content/{filename}.csv')\n",
        "with open('/content/temp.csv', 'w', newline='') as o:\n",
        "    writer = csv.writer(o)\n",
        "    writer.writerow(['location of SNP'])\n",
        "    for loc in snp_locations:\n",
        "        writer.writerow([loc])\n",
        "\n",
        "# Generate URLs for ClinVar based on SNP data\n",
        "rules = []\n",
        "with open('/content/temp.csv', 'r') as t:\n",
        "    reader = csv.reader(t)\n",
        "    next(reader)  # Skip header\n",
        "    for rule in reader:\n",
        "        chr = rule[0].replace('chr', '')\n",
        "        chrpos = chr.split(':')[1].strip() if ':' in chr else ''\n",
        "        chr = chr.split(':')[0]\n",
        "        if \"+\" or \"-\" in chrpos:\n",
        "            chrpos = chrpos.replace(\"+\", \"\").replace(\"-\", \"\")\n",
        "        chrpos = int(chrpos)\n",
        "        chrpos1 = chrpos-3\n",
        "        chrpos2 = chrpos+3\n",
        "        url = f'https://www.ncbi.nlm.nih.gov/clinvar/?term={chr}%5BChr%5D+AND+{chrpos1}%3A{chrpos2}%5BChrPos%5D'\n",
        "#        url = f'https://www.ncbi.nlm.nih.gov/clinvar/?term={chr}%5BChr%5D+AND+{chrpos}%5BChrPos%5D'\n",
        "        with open('/content/input_urls.csv', 'a') as bla:\n",
        "            bla.write(f'{url}\\n')\n",
        "            rules.append(url)\n",
        "\n",
        "# Asynchronous function to fetch a single URL\n",
        "async def fetch_url(session, url):\n",
        "#    debug(f\"Fetching URL: {url}\")\n",
        "    try:\n",
        "        async with session.get(url) as response:\n",
        "            response.raise_for_status()\n",
        "            html = await response.text()\n",
        "#            debug(f\"Response received from {url}\")\n",
        "            soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "            product_links = [\n",
        "                f'https://www.ncbi.nlm.nih.gov{tag[\"href\"]}#clinical_significance'\n",
        "                for tag in soup.find_all(\"a\", href=True) if tag['href'].startswith('/clinvar/rs')\n",
        "            ]\n",
        "#            debug(f\"Found product links: {product_links}\")\n",
        "            return product_links\n",
        "    except Exception as e:\n",
        "        debug(f\"Request failed for {url}: {e}\")\n",
        "        return []\n",
        "\n",
        "# Asynchronous function to fetch all URLs\n",
        "async def fetch_all_urls(urls):\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        tasks = [fetch_url(session, url) for url in urls]\n",
        "        results = await asyncio.gather(*tasks)\n",
        "        return [link for sublist in results for link in sublist]\n",
        "\n",
        "# Main function for fetching ClinVar URLs\n",
        "async def main():\n",
        "    products = await fetch_all_urls(rules)\n",
        "    write_urls_to_csv('/content/input_urls.csv', products)\n",
        "\n",
        "# Run the main coroutine\n",
        "await main()\n",
        "\n",
        "# Function to fetch pathogenic data synchronously\n",
        "def fetch_url_sync(url, retries=3, delay=2):\n",
        "    clinical_terms = [\"Pathogenic\", \"Benign\", \"Conflicting Interpretations Of Pathogenicity\", \"Likely benign\", \"Likely pathogenic\", \"Uncertain significance\"]\n",
        "    debug(f\"Processing URL: {url}\")\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "            html = response.text\n",
        "            soup = BeautifulSoup(html, \"html.parser\")\n",
        "#            debug(f\"Page content length: {len(html)}\")\n",
        "\n",
        "            pathogenic_data = []\n",
        "\n",
        "            # Find all spans with class \"bold\"\n",
        "            span_elements = soup.find_all(\"span\", class_=\"bold\")\n",
        "#            debug(f\"Found {len(span_elements)} span elements with class 'bold'\")\n",
        "\n",
        "            for span in span_elements:\n",
        "                inner_text = span.get_text().strip()\n",
        "                for term in clinical_terms:\n",
        "                    if term in inner_text:\n",
        "                        pathogenic_data.append(f\"{url}:,{inner_text}\")\n",
        "#                        debug(f\"Found clinical term: {inner_text} in {url}\")\n",
        "\n",
        "            return pathogenic_data\n",
        "        except requests.RequestException as e:\n",
        "            debug(f\"Request failed for {url}: {e}\")\n",
        "            time.sleep(delay)\n",
        "            delay *= 2  # Exponential backoff\n",
        "    return []\n",
        "try:\n",
        "  os.remove(f'/content/output_{filename}.csv')\n",
        "except FileNotFoundError: print(f\"File '/content/output_{filename}.csv' not found.\")\n",
        "# Function to fetch all URLs synchronously\n",
        "def fetch_all_urls_sync(urls):\n",
        "    all_data = []\n",
        "    for url in urls:\n",
        "        all_data.extend(fetch_url_sync(url))\n",
        "    return all_data\n",
        "\n",
        "# Main function to process pathogenic data\n",
        "def main_sync():\n",
        "    with open('/content/input_urls.csv', 'r') as inp:\n",
        "        urls = [line.strip() for line in inp.readlines()[1:] if line.strip()]  # Skip empty lines\n",
        "    pathogenic_data = fetch_all_urls_sync(urls)\n",
        "\n",
        "    with open(f'/content/output_{filename}.csv', 'a', newline='') as out_file:\n",
        "        out_file.write('URL:,Clinical significance\\n')\n",
        "        for data in pathogenic_data:\n",
        "            out_file.write(f'{data}\\n')\n",
        "#            debug(f\"Writing data: {data}\")\n",
        "\n",
        "# Run the synchronous main function\n",
        "main_sync()\n"
      ],
      "metadata": {
        "id": "6FcT1isQqs3G",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "###### WORKS WITH RANGE\n",
        "\n",
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import os\n",
        "\n",
        "filename = input(\"insert input file name:\")\n",
        "\n",
        "deviation = input(\"insert deviation from cut site:\")\n",
        "def debug(msg):\n",
        "    print(f\"[DEBUG] {msg}\")\n",
        "try:\n",
        "    os.remove(f'/content/output_{filename}.csv')\n",
        "except FileNotFoundError:\n",
        "    print(f\"File '/content/output_{filename}.csv' not found.\")\n",
        "\n",
        "# Function to read SNP locations from a CSV file (assuming gene and locus data are included in this file)\n",
        "def read_snp_locations(file_path):\n",
        "    snp_data = []\n",
        "    with open(file_path, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        next(reader)  # Skip header\n",
        "        for row in reader:\n",
        "            if row and len(row) >= 2:  # Assuming gene in 1st column, locus in 2nd, SNP in last column\n",
        "                gene = row[-2].strip()\n",
        "                locus = row[-1].strip()\n",
        "                locus = f'{locus} ±{deviation}'\n",
        "                snp_location = row[-1].strip()\n",
        "                snp_data.append((gene, locus, snp_location))\n",
        "    return snp_data\n",
        "\n",
        "# Function to write final output to a CSV file in the format 'gene,locus,url,result'\n",
        "def write_output_to_csv(file_path, data):\n",
        "    with open(file_path, 'a', newline='') as out_file:\n",
        "        writer = csv.writer(out_file)\n",
        "        writer.writerow(['gene', 'locus', 'url', 'result'])  # Write header\n",
        "        for entry in data:\n",
        "            writer.writerow(entry)\n",
        "\n",
        "# Generate URLs for ClinVar based on SNP data\n",
        "def generate_clinvar_urls(snp_data):\n",
        "    urls = []\n",
        "    for gene, locus, snp in snp_data:\n",
        "        chr = snp.replace('chr', '')\n",
        "        chrpos = chr.split(':')[1].strip() if ':' in chr else ''\n",
        "        chr = chr.split(':')[0]\n",
        "        if \"+\" or \"-\" in chrpos:\n",
        "            chrpos = chrpos.replace(\"+\", \"\").replace(\"-\", \"\")\n",
        "        chrpos = int(chrpos)\n",
        "        chrpos1 = chrpos - int(deviation)\n",
        "        chrpos2 = chrpos + int(deviation)\n",
        "        url = f'https://www.ncbi.nlm.nih.gov/clinvar/?term={chr}%5BChr%5D+AND+{chrpos1}%3A{chrpos2}%5BChrPos%5D'\n",
        "        urls.append((gene, locus, url))  # Store gene and locus with URL\n",
        "    return urls\n",
        "\n",
        "# Function to fetch pathogenic data synchronously\n",
        "def fetch_url_sync(gene, locus, url, retries=3, delay=2):\n",
        "    clinical_terms = [\"Pathogenic\", \"Benign\", \"Conflicting Interpretations Of Pathogenicity\", \"Likely benign\", \"Likely pathogenic\", \"Uncertain significance\"]\n",
        "    debug(f\"Processing URL: {url}\")\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "            html = response.text\n",
        "            soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "            # Collect all significant clinical terms\n",
        "            pathogenic_data = []\n",
        "\n",
        "            # Find all spans with class \"bold\" that might contain clinical significance data\n",
        "            span_elements = soup.find_all(\"span\", class_=\"bold\")\n",
        "\n",
        "            for span in span_elements:\n",
        "                inner_text = span.get_text().strip()\n",
        "                for term in clinical_terms:\n",
        "                    if term in inner_text:\n",
        "                        pathogenic_data.append(inner_text)\n",
        "                        break  # Stop checking further terms for this span\n",
        "\n",
        "            # If significant data is found, join it into a single string\n",
        "            if pathogenic_data:\n",
        "                result = \"; \".join(pathogenic_data)\n",
        "                return (gene, locus, url, result)\n",
        "            else:\n",
        "                return None  # Return None if no significant data is found\n",
        "\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"Request failed for {url}: {e}\")\n",
        "            time.sleep(delay)\n",
        "            delay *= 2  # Exponential backoff\n",
        "    return None\n",
        "\n",
        "# Main function to process pathogenic data synchronously\n",
        "def main_sync():\n",
        "    snp_data = read_snp_locations(f'/content/{filename}.csv')\n",
        "    urls = generate_clinvar_urls(snp_data)\n",
        "\n",
        "    output_data = []\n",
        "    for gene, locus, url in urls:\n",
        "        result = fetch_url_sync(gene, locus, url)\n",
        "        if result:  # Only add results that have significant data\n",
        "            output_data.append(result)\n",
        "\n",
        "    write_output_to_csv(f'/content/output_{filename}.csv', output_data)\n",
        "\n",
        "# Run the synchronous main function\n",
        "main_sync()\n"
      ],
      "metadata": {
        "id": "fDdBuwyVxggo",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import os\n",
        "import csv\n",
        "import requests\n",
        "import time\n",
        "import gzip  # Nieuwe import toegevoegd\n",
        "from lxml import html\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service as ChromeService\n",
        "from selenium.webdriver.common.by import By\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "\n",
        "# Installeer de benodigde pakketten (indien nodig)\n",
        "# Uncomment de volgende regels als je deze pakketten nog niet hebt geïnstalleerd\n",
        "# !apt-get update\n",
        "# !apt install -y chromium-browser\n",
        "# !pip install -q selenium\n",
        "# !pip install -q webdriver-manager\n",
        "\n",
        "# Vraag om inputbestand en afwijking\n",
        "filename = input(\"Insert input file name (zonder .csv): \")\n",
        "deviation_input = input(\"Insert deviation from cut site: \")\n",
        "\n",
        "# Converteer deviation naar integer en valideer\n",
        "try:\n",
        "    deviation = int(deviation_input)\n",
        "    if deviation < 0:\n",
        "        raise ValueError(\"Deviation must be a non-negative integer.\")\n",
        "except ValueError as ve:\n",
        "    print(f\"Invalid deviation input: {ve}\")\n",
        "    exit(1)\n",
        "\n",
        "def debug(msg):\n",
        "    print(f\"[DEBUG] {msg}\")\n",
        "\n",
        "# Definieer paden voor outputbestanden\n",
        "output_csv_path = f'output_{filename}.csv'\n",
        "output_sequences_path = f'output_sequences_{filename}.csv'\n",
        "\n",
        "# Verwijder eventueel bestaand outputbestand voor ClinVar gegevens\n",
        "for path in [output_csv_path, output_sequences_path]:\n",
        "    try:\n",
        "        os.remove(path)\n",
        "        print(f\"Removed existing file: {path}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File '{path}' not found. Proceeding...\")\n",
        "\n",
        "# Functie om SNP-locaties uit een CSV-bestand te lezen\n",
        "def read_snp_locations(file_path):\n",
        "    snp_data = []\n",
        "    with open(file_path, 'r', newline='', encoding='utf-8') as f:\n",
        "        reader = csv.reader(f)\n",
        "        header = next(reader, None)  # Sla header over\n",
        "        for row in reader:\n",
        "            if row and len(row) >= 2:  # Aangenomen dat gen in 1e kolom, locus in 2e, SNP in laatste kolom\n",
        "                gene = row[-2].strip()\n",
        "                locus = row[-1].strip()\n",
        "                locus_with_deviation = f'{locus} ±{deviation}'\n",
        "                snp_location = row[-1].strip()\n",
        "                snp_data.append((gene, locus_with_deviation, snp_location))\n",
        "    return snp_data\n",
        "\n",
        "# Functie om output naar CSV te schrijven\n",
        "def write_output_to_csv(file_path, data):\n",
        "    with open(file_path, 'a', newline='', encoding='utf-8') as out_file:\n",
        "        writer = csv.writer(out_file)\n",
        "        if os.path.getsize(file_path) == 0:\n",
        "            writer.writerow(['gene', 'locus', 'url', 'result'])  # Schrijf header alleen als bestand leeg is\n",
        "        for entry in data:\n",
        "            writer.writerow(entry)\n",
        "\n",
        "# Functie om ClinVar URL's te genereren op basis van SNP-data\n",
        "def generate_clinvar_urls(snp_data):\n",
        "    urls = []\n",
        "    for gene, locus, snp in snp_data:\n",
        "        chr_info = snp.replace('chr', '')\n",
        "        if ':' in chr_info:\n",
        "            chr_num, chr_pos = chr_info.split(':', 1)\n",
        "        else:\n",
        "            chr_num, chr_pos = chr_info, ''\n",
        "        chr_num = chr_num.strip()\n",
        "        if \"+\" in chr_pos:\n",
        "            chr_pos = chr_pos.replace(\"+\", \"\").replace(\"-\",\"\")\n",
        "            chr_pos = int(chr_pos) - 3\n",
        "        elif \"-\" in chr_pos:\n",
        "            chr_pos = chr_pos.replace(\"+\", \"\").replace(\"-\",\"\")\n",
        "            chr_pos = int(chr_pos) + 3\n",
        "        try:\n",
        "            chr_pos = int(chr_pos)\n",
        "            if chr_pos <= 0:\n",
        "                print(f\"Invalid chromosome position (non-positive): {chr_pos} for SNP: {snp}\")\n",
        "                continue\n",
        "        except ValueError:\n",
        "            print(f\"Invalid chromosome position: {chr_pos} for SNP: {snp}\")\n",
        "            continue\n",
        "        chrpos1 = chr_pos - deviation\n",
        "        chrpos2 = chr_pos + deviation\n",
        "        # Zorg ervoor dat chrpos1 niet negatief wordt\n",
        "        if chrpos1 < 1:\n",
        "            chrpos1 = 1\n",
        "        url = f'https://www.ncbi.nlm.nih.gov/clinvar/?term={chr_num}%5BChr%5D+AND+{chrpos1}%3A{chrpos2}%5BChrPos%5D'\n",
        "        urls.append((gene, locus, url))  # Sla gen en locus op met URL\n",
        "    return urls\n",
        "\n",
        "# Functie om ClinVar gegevens synchron te halen\n",
        "def fetch_clinvar_data(gene, locus, url, retries=3, delay=2):\n",
        "    clinical_terms = [\"Pathogenic\", \"Benign\", \"Conflicting Interpretations Of Pathogenicity\",\n",
        "                      \"Likely benign\", \"Likely pathogenic\", \"Uncertain significance\"]\n",
        "    debug(f\"Processing URL: {url}\")\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            headers = {\n",
        "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                              \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                              \"Chrome/58.0.3029.110 Safari/537.3\"\n",
        "            }\n",
        "            response = requests.get(url, headers=headers)\n",
        "            response.raise_for_status()\n",
        "            html_content = response.content  # Gebruik bytes in plaats van string\n",
        "            tree = html.fromstring(html_content)\n",
        "\n",
        "            # Zoek naar klinische termen binnen spans met class \"bold\"\n",
        "            pathogenic_data = []\n",
        "            span_elements = tree.xpath(\"//span[@class='bold']\")\n",
        "            for span in span_elements:\n",
        "                inner_text = span.text_content().strip()\n",
        "                for term in clinical_terms:\n",
        "                    if term in inner_text:\n",
        "                        pathogenic_data.append(inner_text)\n",
        "                        break  # Stop met zoeken naar termen voor deze span\n",
        "\n",
        "            # Als significante data is gevonden, voeg deze samen\n",
        "            if pathogenic_data:\n",
        "                result = \"; \".join(pathogenic_data)\n",
        "                return (gene, locus, url, result)\n",
        "            else:\n",
        "                return None  # Geen significante data gevonden\n",
        "\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"Request failed for {url}: {e}\")\n",
        "            time.sleep(delay)\n",
        "            delay *= 2  # Exponentiële backoff\n",
        "        except ValueError as ve:\n",
        "            print(f\"Parsing failed for {url}: {ve}\")\n",
        "            print(\"HTML content causing the issue:\")\n",
        "            print(html_content[:500])  # Print de eerste 500 bytes van de HTML voor debugging\n",
        "            break  # Stop met proberen als er een parsing error is\n",
        "    return None\n",
        "\n",
        "# Hoofdfunctie om ClinVar data synchron te verwerken\n",
        "def main_sync():\n",
        "    snp_data = read_snp_locations(f'{filename}.csv')\n",
        "    urls = generate_clinvar_urls(snp_data)\n",
        "\n",
        "    output_data = []\n",
        "    for gene, locus, url in urls:\n",
        "        result = fetch_clinvar_data(gene, locus, url)\n",
        "        if result:  # Voeg alleen resultaten toe die significante data hebben\n",
        "            output_data.append(result)\n",
        "\n",
        "    write_output_to_csv(output_csv_path, output_data)\n",
        "\n",
        "# Voer de synchron main functie uit voor ClinVar gegevens\n",
        "main_sync()\n",
        "\n",
        "# Initialiseer het output_sequences.csv bestand met header\n",
        "with open(output_sequences_path, 'w', newline='', encoding='utf-8') as seq:\n",
        "    writer = csv.writer(seq)\n",
        "    writer.writerow(['gene', 'strand', 'locus', 'sequence'])  # Schrijf header\n",
        "\n",
        "# Functie om Chromosome URL's te genereren en FASTA-sequenties te halen\n",
        "def generate_chromosome_urls(output):\n",
        "    with open(output, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()[1:]  # Sla header over\n",
        "        for line in lines:\n",
        "            column = line.strip().split(',')\n",
        "            if len(column) < 2:\n",
        "                print(f\"Invalid line format: {line}\")\n",
        "                continue\n",
        "\n",
        "            gene = column[0].strip()\n",
        "            locus_info = column[1].strip()\n",
        "\n",
        "            try:\n",
        "                chr_part, pos_part = locus_info.split(':', 1)\n",
        "                chr_num = chr_part.replace('chr', '').strip()\n",
        "                pos_str = pos_part.strip().split(' ')[0]\n",
        "\n",
        "                if \"+\" in pos_str:\n",
        "                    pos_str_clean = pos_str.replace(\"+\", \"\").replace(\"-\", \"\")\n",
        "                    chrpos = int(pos_str_clean) + 17\n",
        "                    strand = \"+\"\n",
        "                elif \"-\" in pos_str:\n",
        "                    pos_str_clean = pos_str.replace(\"+\", \"\").replace(\"-\", \"\")\n",
        "                    chrpos = int(pos_str_clean) + 3\n",
        "                    strand = \"-\"\n",
        "                else:\n",
        "                    print(f\"Invalid chromosome position format: {pos_str}\")\n",
        "                    continue\n",
        "\n",
        "                if chrpos <= 0:\n",
        "                    print(f\"Invalid chrpos (non-positive): {chrpos} for locus: {locus_info}\")\n",
        "                    continue\n",
        "\n",
        "            except (ValueError, IndexError) as e:\n",
        "                print(f\"Error parsing locus info '{locus_info}': {e}\")\n",
        "                continue\n",
        "\n",
        "            chrpos1 = chrpos - 100\n",
        "            chrpos2 = chrpos  # Originele positie\n",
        "            chrpos3 = chrpos + 101\n",
        "\n",
        "            # Zorg ervoor dat chrpos1 niet negatief wordt\n",
        "            if chrpos1 < 1:\n",
        "                chrpos1 = 1\n",
        "\n",
        "            fasta_url = f'http://hgdownload.soe.ucsc.edu/goldenPath/hg38/chromosomes/chr{chr_num}.fa.gz'\n",
        "\n",
        "            # Yield alle benodigde informatie\n",
        "            yield (fasta_url, chrpos1, chrpos2, chrpos3, chr_num, gene, strand)\n",
        "\n",
        "# Functie om FASTA-bestanden te downloaden en sequenties te extraheren\n",
        "def download_fasta(fasta_urls):\n",
        "    for url, pos1, pos2, pos3, chr_num, gene, strand in fasta_urls:\n",
        "        print(f\"Downloading FASTA from: {url}\")\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"Failed to download {url}: {e}\")\n",
        "            continue\n",
        "\n",
        "        fasta_filename = f'chr{chr_num}.fa.gz'\n",
        "        with open(fasta_filename, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(f\"Downloaded {fasta_filename}.\")\n",
        "\n",
        "        # Extract sequences from the downloaded FASTA\n",
        "        try:\n",
        "            with gzip.open(fasta_filename, 'rt', encoding='utf-8') as f:\n",
        "                sequence = \"\"\n",
        "                for line in f:\n",
        "                    line = line.strip()\n",
        "                    if line.startswith('>'):\n",
        "                        continue  # Skip header lines\n",
        "                    sequence += line  # Concatenate sequence lines\n",
        "        except OSError as e:\n",
        "            print(f\"Failed to extract {fasta_filename}: {e}\")\n",
        "            os.remove(fasta_filename)\n",
        "            continue\n",
        "\n",
        "        # Zorg ervoor dat we de juiste posities gebruiken om de sequenties te extraheren\n",
        "        # Let op: Python is 0-based, dus we passen de indices aan\n",
        "        # Controleer of pos3 niet buiten de sequentie valt\n",
        "        if pos3 > len(sequence):\n",
        "            print(f\"Position {pos3} exceeds sequence length for chr{chr_num}. Skipping.\")\n",
        "            os.remove(fasta_filename)\n",
        "            continue\n",
        "\n",
        "        seq1 = sequence[pos1:pos2]  # Sequentie tussen pos1 en chrpos\n",
        "        seq2 = sequence[pos2:pos3]    # Sequentie tussen chrpos en pos3\n",
        "\n",
        "        # Schrijf beide sequenties naar het output_sequences.csv bestand\n",
        "        with open(output_sequences_path, 'a', newline='', encoding='utf-8') as seq_file:\n",
        "            writer = csv.writer(seq_file)\n",
        "            writer.writerow([gene, strand, f'chr{chr_num}:{pos1}-{pos2}', seq1])  # Eerste sequentie\n",
        "            writer.writerow([gene, strand, f'chr{chr_num}:{pos2}-{pos3}', seq2])      # Tweede sequentie\n",
        "\n",
        "        # Verwijder het gedownloade FASTA-bestand om ruimte te besparen\n",
        "        os.remove(fasta_filename)\n",
        "        print(f\"Processed and removed {fasta_filename}.\")\n",
        "\n",
        "# Genereer chromosoom URL's en download FASTA-bestanden\n",
        "fasta_urls = list(generate_chromosome_urls(output_csv_path))\n",
        "download_fasta(fasta_urls)\n",
        "\n",
        "#print(chrnum)\n",
        "print(\"Process completed.\")\n"
      ],
      "metadata": {
        "id": "uHwB2UVqB5N6",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this cell after uploading your file in the main path on colab and typing in the text spaces"
      ],
      "metadata": {
        "id": "Gb0Ra_6AIka6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import requests\n",
        "import time\n",
        "import gzip\n",
        "from lxml import html\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from collections import defaultdict\n",
        "\n",
        "# Vraag om inputbestand en afwijking\n",
        "file_name = \"insert filename\" #@param {type: \"string\"}\n",
        "deviation = 1 #@param {type: \"integer\"}\n",
        "\n",
        "# Converteer deviation naar integer en valideer\n",
        "try:\n",
        "    deviation = int(deviation)\n",
        "    if deviation < 0:\n",
        "        raise ValueError(\"Deviation must be a non-negative integer.\")\n",
        "except ValueError as ve:\n",
        "    print(f\"Invalid deviation input: {ve}\")\n",
        "    exit(1)\n",
        "\n",
        "# Definieer paden voor outputbestanden\n",
        "\n",
        "output_csv_path = f'output_{file_name}.csv'\n",
        "output_sequences_path = f'output_sequences_{file_name}.csv'\n",
        "\n",
        "# Verwijder eventueel bestaand outputbestand voor ClinVar gegevens\n",
        "for path in [output_csv_path, output_sequences_path]:\n",
        "    try:\n",
        "        os.remove(path)\n",
        "        print(f\"Removed existing file: {path}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File '{path}' not found. Proceeding...\")\n",
        "\n",
        "# Functie om SNP-locaties uit een CSV-bestand te lezen\n",
        "def read_snp_locations(file_path):\n",
        "    snp_data = []\n",
        "    with open(file_path, 'r', newline='', encoding='utf-8') as f:\n",
        "        reader = csv.reader(f)\n",
        "        header = next(reader, None)  # Sla header over\n",
        "        for row in reader:\n",
        "            if row and len(row) >= 2:  # Aangenomen dat gen in 1e kolom, locus in 2e, SNP in laatste kolom\n",
        "                gene = row[-2].strip()\n",
        "                locus = row[-1].strip()\n",
        "                locus_with_deviation = f'{locus} ±{deviation}'\n",
        "                snp_location = row[-1].strip()\n",
        "                snp_data.append((gene, locus_with_deviation, snp_location))\n",
        "    return snp_data\n",
        "\n",
        "# Functie om output naar CSV te schrijven\n",
        "def write_output_to_csv(file_path, data, header=None):\n",
        "    mode = 'a'\n",
        "    write_header = False\n",
        "    if not os.path.exists(file_path):\n",
        "        write_header = True\n",
        "    elif os.path.getsize(file_path) == 0:\n",
        "        write_header = True\n",
        "\n",
        "    with open(file_path, mode, newline='', encoding='utf-8') as out_file:\n",
        "        writer = csv.writer(out_file)\n",
        "        if write_header and header:\n",
        "            writer.writerow(header)\n",
        "        writer.writerows(data)\n",
        "\n",
        "# Functie om ClinVar URL's te genereren op basis van SNP-data\n",
        "def generate_clinvar_urls(snp_data):\n",
        "    urls = []\n",
        "    for gene, locus, snp in snp_data:\n",
        "        chr_info = snp.replace('chr', '')\n",
        "        if ':' in chr_info:\n",
        "            chr_num, chr_pos = chr_info.split(':', 1)\n",
        "        else:\n",
        "            chr_num, chr_pos = chr_info, ''\n",
        "        chr_num = chr_num.strip()\n",
        "        if \"+\" in chr_pos:\n",
        "            chr_pos = chr_pos.replace(\"+\", \"\").replace(\"-\",\"\")\n",
        "            chr_pos = int(chr_pos) + 17\n",
        "        elif \"-\" in chr_pos:\n",
        "            chr_pos = chr_pos.replace(\"+\", \"\").replace(\"-\",\"\")\n",
        "            chr_pos = int(chr_pos) + 3\n",
        "        try:\n",
        "            chr_pos = int(chr_pos)\n",
        "            if chr_pos <= 0:\n",
        "                print(f\"Invalid chromosome position (non-positive): {chr_pos} for SNP: {snp}\")\n",
        "                continue\n",
        "        except ValueError:\n",
        "            print(f\"Invalid chromosome position: {chr_pos} for SNP: {snp}\")\n",
        "            continue\n",
        "        chrpos1 = chr_pos - deviation\n",
        "        chrpos2 = chr_pos + deviation\n",
        "        # Zorg ervoor dat chrpos1 niet negatief wordt\n",
        "        if chrpos1 < 1:\n",
        "            chrpos1 = 1\n",
        "        url = f'https://www.ncbi.nlm.nih.gov/clinvar/?term={chr_num}%5BChr%5D+AND+{chrpos1}%3A{chrpos2}%5BChrPos%5D'\n",
        "        urls.append((gene, locus, url, chr_num, chr_pos))\n",
        "    return urls\n",
        "\n",
        "# Functie om ClinVar gegevens op te halen\n",
        "def fetch_clinvar_data(entry, retries=3, delay=2):\n",
        "    gene, locus, url, chr_num, chr_pos = entry\n",
        "    clinical_terms = [\n",
        "        \"Pathogenic\", \"Benign\", \"Conflicting Interpretations Of Pathogenicity\",\n",
        "        \"Likely benign\", \"Likely pathogenic\", \"Uncertain significance\", \"Oncogenic\"\n",
        "    ]\n",
        "    print(f\"Processing URL: {url}\")\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            headers = {\n",
        "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                              \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                              \"Chrome/58.0.3029.110 Safari/537.3\"\n",
        "            }\n",
        "            response = requests.get(url, headers=headers, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            tree = html.fromstring(response.content)\n",
        "\n",
        "            # Zoek naar klinische termen binnen spans met class \"bold\"\n",
        "            pathogenic_data = []\n",
        "            span_elements = tree.xpath(\"//span[@class='bold']\")\n",
        "            for span in span_elements:\n",
        "                inner_text = span.text_content().strip()\n",
        "                for term in clinical_terms:\n",
        "                    if term in inner_text:\n",
        "                        pathogenic_data.append(inner_text)\n",
        "                        break  # Stop met zoeken naar termen voor deze span\n",
        "\n",
        "            # Als significante data is gevonden, voeg deze samen\n",
        "            if pathogenic_data:\n",
        "                result = \" - \".join(pathogenic_data)\n",
        "                return (gene, locus, url, result)\n",
        "            else:\n",
        "                return None  # Geen significante data gevonden\n",
        "\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"Request failed for {url}: {e}\")\n",
        "            time.sleep(delay)\n",
        "            delay *= 2  # Exponentiële backoff\n",
        "        except ValueError as ve:\n",
        "            print(f\"Parsing failed for {url}: {ve}\")\n",
        "            break  # Stop met proberen als er een parsing error is\n",
        "    return None\n",
        "\n",
        "# Hoofdfunctie om ClinVar data parallel te verwerken\n",
        "def main_sync():\n",
        "    snp_data = read_snp_locations(f'{file_name}.csv')\n",
        "    urls = generate_clinvar_urls(snp_data)\n",
        "\n",
        "    header = ['gene', 'locus', 'url', 'result']\n",
        "    write_output_to_csv(output_csv_path, [], header=header)  # Initialize with header\n",
        "\n",
        "    output_data = []\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        future_to_entry = {executor.submit(fetch_clinvar_data, entry): entry for entry in urls}\n",
        "        for future in as_completed(future_to_entry):\n",
        "            result = future.result()\n",
        "            if result:\n",
        "                output_data.append(result)\n",
        "\n",
        "    # Write all results at once\n",
        "    write_output_to_csv(output_csv_path, output_data)\n",
        "\n",
        "# Initialiseer het output_sequences.csv bestand met header\n",
        "def initialize_sequences_csv():\n",
        "    with open(output_sequences_path, 'w', newline='', encoding='utf-8') as seq:\n",
        "        writer = csv.writer(seq)\n",
        "        writer.writerow(['gene', 'strand', 'locus', 'sequence'])  # Schrijf header\n",
        "\n",
        "# Functie om Chromosome URL's te genereren en SNPs te groeperen per chromosoom\n",
        "def generate_chromosome_snp_map(output):\n",
        "    snp_map = defaultdict(list)\n",
        "    with open(output, 'r', encoding='utf-8') as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for row in reader:\n",
        "            locus = row['locus']\n",
        "            gene = row['gene']\n",
        "            # Parse locus to get chromosome number and position\n",
        "            try:\n",
        "                chr_part, pos_part = locus.split(':', 1)\n",
        "                chr_num = chr_part.replace('chr', '').strip()\n",
        "                pos_str = pos_part.strip().split(' ')[0]\n",
        "\n",
        "                if \"+\" in pos_str:\n",
        "                    pos_str_clean = pos_str.replace(\"+\", \"\").replace(\"-\", \"\")\n",
        "                    chrpos = int(pos_str_clean) + 17\n",
        "                    strand = \"+\"\n",
        "                elif \"-\" in pos_str:\n",
        "                    pos_str_clean = pos_str.replace(\"+\", \"\").replace(\"-\", \"\")\n",
        "                    chrpos = int(pos_str_clean) + 3\n",
        "                    strand = \"-\"\n",
        "                else:\n",
        "                    print(f\"Invalid chromosome position format: {pos_str}\")\n",
        "                    continue\n",
        "\n",
        "                if chrpos <= 0:\n",
        "                    print(f\"Invalid chrpos (non-positive): {chrpos} for locus: {locus}\")\n",
        "                    continue\n",
        "\n",
        "                chrpos1 = chrpos - 100\n",
        "                chrpos2 = chrpos  # Originele positie\n",
        "                chrpos3 = chrpos + 101\n",
        "\n",
        "                # Zorg ervoor dat chrpos1 niet negatief wordt\n",
        "                if chrpos1 < 1:\n",
        "                    chrpos1 = 1\n",
        "\n",
        "                snp_map[chr_num].append({\n",
        "                    'gene': gene,\n",
        "                    'strand': strand,\n",
        "                    'chrpos1': chrpos1,\n",
        "                    'chrpos2': chrpos2,\n",
        "                    'chrpos3': chrpos3,\n",
        "                    'locus': locus\n",
        "                })\n",
        "\n",
        "            except (ValueError, IndexError) as e:\n",
        "                print(f\"Error parsing locus info '{locus}': {e}\")\n",
        "                continue\n",
        "    return snp_map\n",
        "\n",
        "# Functie om FASTA-bestanden te downloaden en sequenties te extraheren\n",
        "def download_and_process_fasta(chr_num, snps):\n",
        "    fasta_url = f'http://hgdownload.soe.ucsc.edu/goldenPath/hg38/chromosomes/chr{chr_num}.fa.gz'\n",
        "    fasta_filename = f'chr{chr_num}.fa.gz'\n",
        "\n",
        "    print(f\"Downloading FASTA from: {fasta_url}\")\n",
        "    try:\n",
        "        response = requests.get(fasta_url, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        with open(fasta_filename, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(f\"Downloaded {fasta_filename}.\")\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Failed to download {fasta_url}: {e}\")\n",
        "        return\n",
        "\n",
        "    # Extract sequences from the downloaded FASTA\n",
        "    try:\n",
        "        with gzip.open(fasta_filename, 'rt', encoding='utf-8') as f:\n",
        "            sequence = []\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line.startswith('>'):\n",
        "                    continue  # Skip header lines\n",
        "                sequence.append(line)\n",
        "            sequence = ''.join(sequence)\n",
        "    except OSError as e:\n",
        "        print(f\"Failed to extract {fasta_filename}: {e}\")\n",
        "        os.remove(fasta_filename)\n",
        "        return\n",
        "\n",
        "    # Verwijder het gedownloade FASTA-bestand om ruimte te besparen\n",
        "    os.remove(fasta_filename)\n",
        "    print(f\"Processed and removed {fasta_filename}.\")\n",
        "\n",
        "    # Extract and collect sequences\n",
        "    sequences = []\n",
        "    for snp in snps:\n",
        "        gene = snp['gene']\n",
        "        strand = snp['strand']\n",
        "        pos1 = snp['chrpos1']\n",
        "        pos2 = snp['chrpos2']\n",
        "        pos3 = snp['chrpos3']\n",
        "        locus = snp['locus']\n",
        "\n",
        "        # Controleer of pos3 niet buiten de sequentie valt\n",
        "        if pos3 > len(sequence):\n",
        "            print(f\"Position {pos3} exceeds sequence length for chr{chr_num}. Skipping SNP at {locus}.\")\n",
        "            continue\n",
        "\n",
        "        # Python is 0-based, dus we passen de indices aan\n",
        "        seq1 = sequence[pos1:pos2]  # Sequentie tussen pos1 en chrpos\n",
        "        seq2 = sequence[pos2:pos3]    # Sequentie tussen chrpos en pos3\n",
        "\n",
        "        sequences.append([gene, strand, f'chr{chr_num}:{pos1}-{pos2}', seq1])  # Eerste sequentie\n",
        "        sequences.append([gene, strand, f'chr{chr_num}:{pos2}-{pos3}', seq2])      # Tweede sequentie\n",
        "\n",
        "    # Schrijf alle sequenties in één keer naar het CSV-bestand\n",
        "    write_output_to_csv(output_sequences_path, sequences)\n",
        "\n",
        "# Hoofdfunctie om FASTA-bestanden parallel te downloaden en te verwerken\n",
        "def process_fasta_sequences(snp_map):\n",
        "    initialize_sequences_csv()\n",
        "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        futures = []\n",
        "        for chr_num, snps in snp_map.items():\n",
        "            futures.append(executor.submit(download_and_process_fasta, chr_num, snps))\n",
        "        for future in as_completed(futures):\n",
        "            future.result()  # Trigger exceptions if any\n",
        "\n",
        "# Hoofdfunctie om het gehele proces uit te voeren\n",
        "def main():\n",
        "    start_time = time.time()\n",
        "    print(\"Starting ClinVar data retrieval...\")\n",
        "    main_sync()\n",
        "    print(\"ClinVar data retrieval completed.\")\n",
        "\n",
        "    print(\"Grouping SNPs by chromosome for FASTA processing...\")\n",
        "    snp_map = generate_chromosome_snp_map(output_csv_path)\n",
        "    print(f\"Grouped SNPs into {len(snp_map)} chromosomes.\")\n",
        "\n",
        "    print(\"Starting FASTA downloads and sequence extraction...\")\n",
        "    process_fasta_sequences(snp_map)\n",
        "    print(\"FASTA processing completed.\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Process completed in {elapsed_time:.2f} seconds.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUCXzmbDEBHf",
        "outputId": "4cf6e804-da9f-4fdc-9812-2b54fab0bb94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'output_All chromosomes.csv' not found. Proceeding...\n",
            "File 'output_sequences_All chromosomes.csv' not found. Proceeding...\n",
            "Starting ClinVar data retrieval...\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=1%5BChr%5D+AND+198699578%3A198699618%5BChrPos%5DProcessing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=1%5BChr%5D+AND+155235235%3A155235275%5BChrPos%5D\n",
            "\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=2%5BChr%5D+AND+47800613%3A47800653%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=3%5BChr%5D+AND+179218300%3A179218340%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=4%5BChr%5D+AND+1805641%3A1805681%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=5%5BChr%5D+AND+223506%3A223546%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=6%5BChr%5D+AND+43007262%3A43007302%5BChrPos%5DProcessing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=7%5BChr%5D+AND+117530972%3A117531012%5BChrPos%5D\n",
            "\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=8%5BChr%5D+AND+74364002%3A74364042%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=9%5BChr%5D+AND+95172016%3A95172056%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=10%5BChr%5D+AND+43120141%3A43120181%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=11%5BChr%5D+AND+47342561%3A47342601%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=12%5BChr%5D+AND+102851684%3A102851724%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=13%5BChr%5D+AND+32370952%3A32370992%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=2%5BChr%5D+AND+217318024%3A217318064%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=14%5BChr%5D+AND+45189120%3A45189160%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=15%5BChr%5D+AND+89327184%3A89327224%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=22%5BChr%5D+AND+29167955%3A29167995%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=16%5BChr%5D+AND+3243430%3A3243470%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=17%5BChr%5D+AND+7674876%3A7674916%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=18%5BChr%5D+AND+31598652%3A31598692%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=19%5BChr%5D+AND+11116122%3A11116162%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=20%5BChr%5D+AND+58909362%3A58909402%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=21%5BChr%5D+AND+34859469%3A34859509%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=22%5BChr%5D+AND+28725225%3A28725265%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=X%5BChr%5D+AND+154031414%3A154031454%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=Y%5BChr%5D+AND+2787384%3A2787424%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=19%5BChr%5D+AND+1281040%3A1281080%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=12%5BChr%5D+AND+120658755%3A120658795%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=3%5BChr%5D+AND+12923571%3A12923611%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=9%5BChr%5D+AND+95353449%3A95353489%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=10%5BChr%5D+AND+49254029%3A49254069%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=17%5BChr%5D+AND+67673375%3A67673415%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=11%5BChr%5D+AND+61497584%3A61497624%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=9%5BChr%5D+AND+134764633%3A134764673%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=9%5BChr%5D+AND+79115226%3A79115266%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=12%5BChr%5D+AND+1841206%3A1841246%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=1%5BChr%5D+AND+244173404%3A244173444%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=7%5BChr%5D+AND+74217817%3A74217857%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=10%5BChr%5D+AND+1853227%3A1853267%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=13%5BChr%5D+AND+26944357%3A26944397%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=12%5BChr%5D+AND+55241960%3A55242000%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=8%5BChr%5D+AND+14739106%3A14739146%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=X%5BChr%5D+AND+80983883%3A80983923%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=12%5BChr%5D+AND+130573430%3A130573470%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=1%5BChr%5D+AND+1177014%3A1177054%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=19%5BChr%5D+AND+39550603%3A39550643%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=7%5BChr%5D+AND+5470060%3A5470100%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=19%5BChr%5D+AND+7672069%3A7672109%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=1%5BChr%5D+AND+40886765%3A40886805%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=15%5BChr%5D+AND+81843239%3A81843279%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=9%5BChr%5D+AND+38212774%3A38212814%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=11%5BChr%5D+AND+127449126%3A127449166%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=11%5BChr%5D+AND+1252967%3A1253007%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=1%5BChr%5D+AND+227885740%3A227885780%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=8%5BChr%5D+AND+57419087%3A57419127%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=15%5BChr%5D+AND+40112939%3A40112979%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=7%5BChr%5D+AND+5295236%3A5295276%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=16%5BChr%5D+AND+85650820%3A85650860%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=17%5BChr%5D+AND+812812%3A812852%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=22%5BChr%5D+AND+23295348%3A23295388%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=X%5BChr%5D+AND+71757244%3A71757284%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=6%5BChr%5D+AND+99190304%3A99190344%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=X%5BChr%5D+AND+71724242%3A71724282%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=3%5BChr%5D+AND+195887842%3A195887882%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=16%5BChr%5D+AND+88645343%3A88645383%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=X%5BChr%5D+AND+84862152%3A84862192%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=2%5BChr%5D+AND+122538470%3A122538510%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=4%5BChr%5D+AND+23361721%3A23361761%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=2%5BChr%5D+AND+129700399%3A129700439%5BChrPos%5DProcessing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=18%5BChr%5D+AND+3065638%3A3065678%5BChrPos%5D\n",
            "\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=10%5BChr%5D+AND+70960412%3A70960452%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=17%5BChr%5D+AND+79601917%3A79601957%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=11%5BChr%5D+AND+132036997%3A132037037%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=20%5BChr%5D+AND+63540123%3A63540163%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=16%5BChr%5D+AND+84822503%3A84822543%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=17%5BChr%5D+AND+76982738%3A76982778%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=18%5BChr%5D+AND+23672634%3A23672674%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=20%5BChr%5D+AND+25118818%3A25118858%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=8%5BChr%5D+AND+144223330%3A144223370%5BChrPos%5DProcessing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=11%5BChr%5D+AND+2548781%3A2548821%5BChrPos%5D\n",
            "\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=20%5BChr%5D+AND+32460664%3A32460704%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=11%5BChr%5D+AND+128721598%3A128721638%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=15%5BChr%5D+AND+37037426%3A37037466%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=4%5BChr%5D+AND+146633093%3A146633133%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=6%5BChr%5D+AND+3727979%3A3728019%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=20%5BChr%5D+AND+63253887%3A63253927%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=14%5BChr%5D+AND+100707184%3A100707224%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=X%5BChr%5D+AND+17533213%3A17533253%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=4%5BChr%5D+AND+131730521%3A131730561%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=9%5BChr%5D+AND+101169180%3A101169220%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=4%5BChr%5D+AND+166001413%3A166001453%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=5%5BChr%5D+AND+2659703%3A2659743%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=9%5BChr%5D+AND+136335053%3A136335093%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=10%5BChr%5D+AND+46538368%3A46538408%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=20%5BChr%5D+AND+59994344%3A59994384%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=15%5BChr%5D+AND+75023761%3A75023801%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=2%5BChr%5D+AND+113820576%3A113820616%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=14%5BChr%5D+AND+40260072%3A40260112%5BChrPos%5D\n",
            "Processing URL: https://www.ncbi.nlm.nih.gov/clinvar/?term=6%5BChr%5D+AND+144387889%3A144387929%5BChrPos%5D\n",
            "ClinVar data retrieval completed.\n",
            "Grouping SNPs by chromosome for FASTA processing...\n",
            "Grouped SNPs into 24 chromosomes.\n",
            "Starting FASTA downloads and sequence extraction...\n",
            "Downloading FASTA from: http://hgdownload.soe.ucsc.edu/goldenPath/hg38/chromosomes/chr1.fa.gzDownloading FASTA from: http://hgdownload.soe.ucsc.edu/goldenPath/hg38/chromosomes/chr3.fa.gz\n",
            "\n",
            "Downloading FASTA from: http://hgdownload.soe.ucsc.edu/goldenPath/hg38/chromosomes/chr8.fa.gz\n",
            "Downloading FASTA from: http://hgdownload.soe.ucsc.edu/goldenPath/hg38/chromosomes/chr6.fa.gz\n",
            "Downloading FASTA from: http://hgdownload.soe.ucsc.edu/goldenPath/hg38/chromosomes/chr5.fa.gz\n",
            "Downloaded chr8.fa.gz.\n",
            "Downloaded chr6.fa.gz.\n",
            "Downloaded chr5.fa.gz.\n",
            "Downloaded chr3.fa.gz.\n",
            "Downloaded chr1.fa.gz.\n",
            "Processed and removed chr8.fa.gz.\n",
            "Downloading FASTA from: http://hgdownload.soe.ucsc.edu/goldenPath/hg38/chromosomes/chr9.fa.gz\n",
            "Downloaded chr9.fa.gz.\n",
            "Processed and removed chr6.fa.gz.\n",
            "Downloading FASTA from: http://hgdownload.soe.ucsc.edu/goldenPath/hg38/chromosomes/chr7.fa.gz\n",
            "Processed and removed chr5.fa.gz.\n",
            "Downloading FASTA from: http://hgdownload.soe.ucsc.edu/goldenPath/hg38/chromosomes/chr4.fa.gz\n",
            "Downloaded chr7.fa.gz.\n",
            "Processed and removed chr3.fa.gz.\n",
            "Downloading FASTA from: http://hgdownload.soe.ucsc.edu/goldenPath/hg38/chromosomes/chr2.fa.gz\n",
            "Downloaded chr4.fa.gz.\n",
            "Downloaded chr2.fa.gz.\n",
            "Processed and removed chr1.fa.gz.\n",
            "Downloading FASTA from: http://hgdownload.soe.ucsc.edu/goldenPath/hg38/chromosomes/chr10.fa.gz\n",
            "Downloaded chr10.fa.gz.\n",
            "Processed and removed chr9.fa.gz.\n",
            "Downloading FASTA from: http://hgdownload.soe.ucsc.edu/goldenPath/hg38/chromosomes/chr11.fa.gz\n",
            "Downloaded chr11.fa.gz.\n",
            "Processed and removed chr7.fa.gz.\n",
            "Downloading FASTA from: http://hgdownload.soe.ucsc.edu/goldenPath/hg38/chromosomes/chr12.fa.gz\n",
            "Downloaded chr12.fa.gz.\n",
            "Processed and removed chr4.fa.gz.\n",
            "Downloading FASTA from: http://hgdownload.soe.ucsc.edu/goldenPath/hg38/chromosomes/chr15.fa.gz\n",
            "Processed and removed chr10.fa.gz.\n",
            "Downloading FASTA from: http://hgdownload.soe.ucsc.edu/goldenPath/hg38/chromosomes/chr16.fa.gz\n",
            "Downloaded chr15.fa.gz.\n",
            "Downloaded chr16.fa.gz.\n",
            "Processed and removed chr11.fa.gz.\n",
            "Downloading FASTA from: http://hgdownload.soe.ucsc.edu/goldenPath/hg38/chromosomes/chr13.fa.gz\n",
            "Downloaded chr13.fa.gz.\n",
            "Processed and removed chr2.fa.gz.\n",
            "Downloading FASTA from: http://hgdownload.soe.ucsc.edu/goldenPath/hg38/chromosomes/chr14.fa.gz\n",
            "Downloaded chr14.fa.gz.\n",
            "Processed and removed chr12.fa.gz.\n",
            "Downloading FASTA from: http://hgdownload.soe.ucsc.edu/goldenPath/hg38/chromosomes/chr17.fa.gz\n",
            "Downloaded chr17.fa.gz.\n",
            "Processed and removed chr16.fa.gz.\n",
            "Downloading FASTA from: http://hgdownload.soe.ucsc.edu/goldenPath/hg38/chromosomes/chr19.fa.gz\n",
            "Downloaded chr19.fa.gz.\n",
            "Processed and removed chr15.fa.gz.\n",
            "Downloading FASTA from: http://hgdownload.soe.ucsc.edu/goldenPath/hg38/chromosomes/chr20.fa.gz\n",
            "Downloaded chr20.fa.gz.\n",
            "Processed and removed chr13.fa.gz.\n",
            "Downloading FASTA from: http://hgdownload.soe.ucsc.edu/goldenPath/hg38/chromosomes/chr21.fa.gz\n",
            "Downloaded chr21.fa.gz.\n",
            "Processed and removed chr19.fa.gz.\n",
            "Downloading FASTA from: http://hgdownload.soe.ucsc.edu/goldenPath/hg38/chromosomes/chr18.fa.gz\n",
            "Downloaded chr18.fa.gz.\n",
            "Processed and removed chr17.fa.gz.\n",
            "Downloading FASTA from: http://hgdownload.soe.ucsc.edu/goldenPath/hg38/chromosomes/chrY.fa.gz\n",
            "Processed and removed chr14.fa.gz.\n",
            "Downloading FASTA from: http://hgdownload.soe.ucsc.edu/goldenPath/hg38/chromosomes/chr22.fa.gz\n",
            "Processed and removed chr20.fa.gz.\n",
            "Downloading FASTA from: http://hgdownload.soe.ucsc.edu/goldenPath/hg38/chromosomes/chrX.fa.gz\n",
            "Downloaded chrY.fa.gz.\n",
            "Downloaded chr22.fa.gz.\n",
            "Downloaded chrX.fa.gz.\n",
            "Processed and removed chr21.fa.gz.\n",
            "Processed and removed chrY.fa.gz.\n",
            "Processed and removed chr22.fa.gz.\n",
            "Processed and removed chr18.fa.gz.\n",
            "Processed and removed chrX.fa.gz.\n",
            "FASTA processing completed.\n",
            "Process completed in 90.30 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code can run within 100 seconds when it has a single pathogenic variant found in each chromosome and when it has 100 lines as input file. (Running on CPU)"
      ],
      "metadata": {
        "id": "I4c8NQuXwOO_"
      }
    }
  ]
}